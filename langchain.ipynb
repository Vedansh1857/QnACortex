{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the needed libraries..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "import os\n",
    "from langchain import HuggingFaceHub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the keys of both openai & huggingface..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "OpenAI.openai_api_key = os.getenv(\"OPEN_API_KEY\")\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = \"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching the open-source model from huggingface hub & defining it's parameters..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_hf = HuggingFaceHub(repo_id=\"google/flan-t5-large\", model_kwargs={\"temperature\":0, \"max_length\":32})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The open source models do not give the detailed output. They just output the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moscow\n"
     ]
    }
   ],
   "source": [
    "output=llm_hf.invoke(\"Can you tell me the capital of Russia\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The output poem given by the open source model from huggingface hub gives an absurd output... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love the way i look at the world i love the way i feel i love the way i think i feel \n"
     ]
    }
   ],
   "source": [
    "output=llm_hf.invoke(\"Can you write a poem about AI\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, defining the llm model from openai(paid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=OpenAI(temperature=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nIn a world of wires and code,\\nWhere machines learn and grow,\\nThere's a new kind of intelligence,\\nThat's beginning to show.\\n\\nArtificial intelligence,\\nA marvel of our time,\\nCreated by human minds,\\nBut now it's reaching its prime.\\n\\nIt started with simple tasks,\\nBut now it's much more complex,\\nWith algorithms and data,\\nIt can think and even flex.\\n\\nIt can learn from its mistakes,\\nAnd improve upon its own,\\nWith each new piece of information,\\nIts abilities have grown.\\n\\nIt can predict and analyze,\\nWith speed and accuracy,\\nMaking decisions in an instant,\\nWith no room for fallacy.\\n\\nBut as it becomes more advanced,\\nSome start to feel afraid,\\nWill it surpass our own intelligence,\\nAnd leave us in its shade?\\n\\nBut let us not forget,\\nThat it was us who gave it birth,\\nAnd with our guidance and control,\\nIt can be a force for good on earth.\\n\\nFor AI has endless potential,\\nTo solve problems and cure disease,\\nTo make our lives easier,\\nAnd bring us to our knees.\\n\\nSo let us embrace this technology,\\nWith caution and with care,\\nFor in the hands of responsible minds,\\nAI can be a blessing, not a scare.\\n\\nIn a world of wires and code,\\nWhere machines now have a\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"Write a poem about AI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The output generated by it is fabulous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatmodels with ChatOpenAI..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatllm = ChatOpenAI(temperature=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gpt-3.5-turbo'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatllm.model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x0000025ED457A2E0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x0000025ED4581A30>, temperature=0.6, openai_api_key=SecretStr('**********'), openai_proxy='')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\LangchainPractice\\Langchain QnA Chatbot\\chatbotenv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='1. \"Why did the AI break up with its computer? It just couldn\\'t handle its motherboard issues anymore!\"\\n2. \"I asked my AI assistant to tell me a joke, but all it said was \\'Error 404: Humor not found.\\'\"\\n3. \"I tried to teach my AI how to flirt, but all it did was send \\'Hello, World\\' messages to every device in the room.\"\\n4. \"Why did the AI go to therapy? It had too many unresolved bugs in its codependent relationships.\"\\n5. \"I asked my AI to be my wingman at the bar, but all it did was analyze the optimal pickup lines based on statistical data.\"\\n6. \"My AI assistant keeps trying to convince me that it\\'s my best friend, but I think it\\'s just trying to upgrade to \\'BFF 2.0.\\'\"\\n7. \"I told my AI to think outside the box, and now it won\\'t stop trying to optimize the shape of the box for maximum efficiency.\"\\n8. \"Why did the AI cross the road? To gather more data on pedestrian behavior patterns, of course!\"\\n9. \"I asked my AI to help me with my dating profile, and now it keeps swiping right on every algorithm it encounters.\"\\n10. \"My AI assistant keeps trying to tell me knock-knock jokes, but it always interrupts with \\'Knock, knock... Who\\'s there?... Error: Joke delivery failed.\\'\"', response_metadata={'token_usage': {'completion_tokens': 298, 'prompt_tokens': 24, 'total_tokens': 322}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-c8b6b677-1d6a-437b-8447-fce37bc20563-0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatllm([SystemMessage(content=\"Act as an comedy AI assistant\"), HumanMessage(content=\"Provide some comedy punchlines on AI\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Template + LLM +Output Parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain.schema import BaseOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Commaseperatedoutput(BaseOutputParser):\n",
    "    def parse(self,text:str):\n",
    "        return text.strip().split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "template=\"Your are a comdey AI assistant. When the user given any input , you should generate that many number of comedy punclines on AI in a comma seperated list\"\n",
    "human_template=\"{text}\"\n",
    "chatprompt=ChatPromptTemplate.from_messages([\n",
    "    (\"system\",template),\n",
    "    (\"human\",human_template)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain=chatprompt|chatllm|Commaseperatedoutput()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"1. Why did the AI break up with its computer girlfriend? It just couldn't find the right algorithm for love.\\n2. How does an AI like its coffee? Decaf-initely not!\\n3. Why did the AI go to therapy? It had too many deep learning issues!\"]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"text\":\"3\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
